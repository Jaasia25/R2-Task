{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81839fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "image_path = \"../data/images/test/00000000_jpg.rf.5d9188ebfd1f9ae1b9989c8ffc6bd7c4.jpg\"\n",
    "category = \"shark\"\n",
    "classes = ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n",
    "model_path = \"../models/fish_yolov86/weights/best.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fada807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO(model_path)\n",
    "model.eval()  # inference mode\n",
    "\n",
    "# Get device (CPU or GPU)\n",
    "device = next(model.model.parameters()).device\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5f4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_resized = img.resize((640, 640))\n",
    "    x = torch.tensor(np.array(img_resized)).permute(2,0,1).unsqueeze(0).float() / 255.0\n",
    "    x.requires_grad = True\n",
    "    return x, np.array(img)\n",
    "\n",
    "x, orig_img = preprocess_image(image_path)\n",
    "x = x.to(device)  # move input to same device as model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf29f6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7f44ddfa6310>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps = {}\n",
    "gradients = {}\n",
    "\n",
    "# Pick a YOLOv8 backbone conv layer (first conv block)\n",
    "target_layer = model.model.model[0]\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    feature_maps['hooked'] = output\n",
    "\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    gradients['hooked'] = grad_out[0]\n",
    "\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_backward_hook(backward_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aaaf7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'shark' confidence score: 0.9030\n"
     ]
    }
   ],
   "source": [
    "# Run model.predict() to get real detection scores\n",
    "dets = model.predict(image_path, verbose=False)[0]  # first image\n",
    "boxes = dets.boxes\n",
    "obj_scores = boxes.conf\n",
    "class_indices = boxes.cls\n",
    "\n",
    "class_idx = classes.index(category)\n",
    "mask = (class_indices == class_idx)\n",
    "\n",
    "if mask.sum() > 0:\n",
    "    score = (obj_scores[mask]).max().item()\n",
    "else:\n",
    "    score = 0.0\n",
    "\n",
    "print(f\"Category '{category}' confidence score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde6a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.1.conv Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.2.cv1.conv Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.2.cv2.conv Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.2.m.0.cv1.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.2.m.0.cv2.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.2.m.1.cv1.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.2.m.1.cv2.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.3.conv Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.4.cv1.conv Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.4.cv2.conv Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.4.m.0.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.0.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.1.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.1.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.2.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.2.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.3.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.3.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.5.conv Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.6.cv1.conv Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.6.cv2.conv Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.6.m.0.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.0.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.1.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.1.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.2.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.2.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.3.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.3.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.7.conv Conv2d(384, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.8.cv1.conv Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.8.cv2.conv Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.8.m.0.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.8.m.0.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.8.m.1.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.8.m.1.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.9.cv1.conv Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.9.cv2.conv Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.12.cv1.conv Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.12.cv2.conv Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.12.m.0.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.12.m.0.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.12.m.1.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.12.m.1.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.cv1.conv Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.15.cv2.conv Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.15.m.0.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.m.0.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.m.1.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.m.1.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.16.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.18.cv1.conv Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.18.cv2.conv Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.18.m.0.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.18.m.0.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.18.m.1.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.18.m.1.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.19.conv Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.21.cv1.conv Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.21.cv2.conv Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.21.m.0.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.21.m.0.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.21.m.1.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.21.m.1.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.0.0.conv Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.0.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.0.2 Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv2.1.0.conv Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.1.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.1.2 Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv2.2.0.conv Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.2.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.2.2 Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv3.0.0.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.0.1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.0.2 Conv2d(192, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv3.1.0.conv Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.1.1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.1.2 Conv2d(192, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv3.2.0.conv Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.2.1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.2.2 Conv2d(192, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.dfl.conv Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.model.named_modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(name, layer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
