{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81839fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "image_path = \"../data/images/test/00000000_jpg.rf.5d9188ebfd1f9ae1b9989c8ffc6bd7c4.jpg\"\n",
    "category = \"shark\"\n",
    "classes = ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n",
    "model_path = \"../models/fish_yolov86/weights/best.pt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fada807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load YOLOv8 model\n",
    "model = YOLO(model_path)\n",
    "model.eval()  # inference mode\n",
    "\n",
    "# Get device (CPU or GPU)\n",
    "device = next(model.model.parameters()).device\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e5f4c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_resized = img.resize((640, 640))\n",
    "    x = torch.tensor(np.array(img_resized)).permute(2,0,1).unsqueeze(0).float() / 255.0\n",
    "    x.requires_grad = True\n",
    "    return x, np.array(img)\n",
    "\n",
    "x, orig_img = preprocess_image(image_path)\n",
    "x = x.to(device)  # move input to same device as model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf29f6bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7fca45b5ea10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps = {}\n",
    "gradients = {}\n",
    "\n",
    "# Pick a YOLOv8 backbone conv layer (first conv block)\n",
    "target_layer = model.model.model[0]\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    feature_maps['hooked'] = output\n",
    "\n",
    "def backward_hook(module, grad_in, grad_out):\n",
    "    gradients['hooked'] = grad_out[0]\n",
    "\n",
    "target_layer.register_forward_hook(forward_hook)\n",
    "target_layer.register_backward_hook(backward_hook)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aaaf7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category 'shark' confidence score: 0.9030\n"
     ]
    }
   ],
   "source": [
    "# Run model.predict() to get real detection scores\n",
    "dets = model.predict(image_path, verbose=False)[0]  # first image\n",
    "boxes = dets.boxes\n",
    "obj_scores = boxes.conf\n",
    "class_indices = boxes.cls\n",
    "\n",
    "class_idx = classes.index(category)\n",
    "mask = (class_indices == class_idx)\n",
    "\n",
    "if mask.sum() > 0:\n",
    "    score = (obj_scores[mask]).max().item()\n",
    "else:\n",
    "    score = 0.0\n",
    "\n",
    "print(f\"Category '{category}' confidence score: {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "577c1e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jassia/workspace/Jaasia/Fish_Detection/fish/lib/python3.11/site-packages/torch/nn/modules/module.py:1866: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n"
     ]
    }
   ],
   "source": [
    "# Move input tensor to the same device as the model\n",
    "device = next(model.model.parameters()).device\n",
    "x = x.to(device)\n",
    "\n",
    "# Forward pass through YOLO model\n",
    "outputs = model.model(x)\n",
    "\n",
    "# Backward pass: ensure scalar_score is on same device\n",
    "model.model.zero_grad()\n",
    "scalar_score = torch.tensor(score, requires_grad=True).to(device)\n",
    "scalar_score.backward(retain_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d86e54",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot use both regular backward hooks and full backward hooks on a single Module. Please use only one of them.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Register hooks\u001b[39;00m\n\u001b[32m     18\u001b[39m f_hook = target_layer.register_forward_hook(forward_hook)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m b_hook = \u001b[43mtarget_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_full_backward_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbackward_hook\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Move input to same device as model\u001b[39;00m\n\u001b[32m     22\u001b[39m device = \u001b[38;5;28mnext\u001b[39m(model.model.parameters()).device\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/Jaasia/Fish_Detection/fish/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule.register_full_backward_hook\u001b[39m\u001b[34m(self, hook, prepend)\u001b[39m\n\u001b[32m   1453\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Register a backward hook on the module.\u001b[39;00m\n\u001b[32m   1454\u001b[39m \n\u001b[32m   1455\u001b[39m \u001b[33;03mThe hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1498\u001b[39m \n\u001b[32m   1499\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_full_backward_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot use both regular backward hooks and full backward hooks on a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msingle Module. Please use only one of them.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1504\u001b[39m     )\n\u001b[32m   1506\u001b[39m \u001b[38;5;28mself\u001b[39m._is_full_backward_hook = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1508\u001b[39m handle = RemovableHandle(\u001b[38;5;28mself\u001b[39m._backward_hooks)\n",
      "\u001b[31mRuntimeError\u001b[39m: Cannot use both regular backward hooks and full backward hooks on a single Module. Please use only one of them."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d0a298",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hooked'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m fmap = feature_maps[\u001b[33m'\u001b[39m\u001b[33mhooked\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# [C,H,W]\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m grad = \u001b[43mgradients\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhooked\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m]     \u001b[38;5;66;03m# [C,H,W]\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Global average pooling of gradients\u001b[39;00m\n\u001b[32m      5\u001b[39m weights = torch.mean(grad, dim=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m))\n",
      "\u001b[31mKeyError\u001b[39m: 'hooked'"
     ]
    }
   ],
   "source": [
    "fmap = feature_maps['hooked'][0]  # [C,H,W]\n",
    "grad = gradients['hooked'][0]     # [C,H,W]\n",
    "\n",
    "# Global average pooling of gradients\n",
    "weights = torch.mean(grad, dim=(1,2))\n",
    "\n",
    "# Weighted combination of feature maps\n",
    "gradcam_map = torch.zeros(fmap.shape[1:], dtype=torch.float32, device=device)\n",
    "for i in range(weights.shape[0]):\n",
    "    gradcam_map += weights[i] * fmap[i]\n",
    "\n",
    "# ReLU and normalize\n",
    "gradcam_map = torch.relu(gradcam_map)\n",
    "gradcam_map -= gradcam_map.min()\n",
    "gradcam_map /= gradcam_map.max()\n",
    "\n",
    "# Resize to original image size\n",
    "gradcam_resized = cv2.resize(gradcam_map.detach().cpu().numpy(), (orig_img.shape[1], orig_img.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f9b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_heatmap(img, heatmap, alpha=0.4):\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    superimposed_img = cv2.addWeighted(img, alpha, heatmap, 1 - alpha, 0)\n",
    "    return superimposed_img\n",
    "\n",
    "output_img = overlay_heatmap(orig_img, gradcam_resized)\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(output_img)\n",
    "plt.axis('off')\n",
    "plt.title(f\"GradCAM: {category}, Score: {score:.4f}\")\n",
    "plt.show()\n",
    "\n",
    "# Optional: save\n",
    "cv2.imwrite(\"heatmap/yolov8_gradcam.jpg\", cv2.cvtColor(output_img, cv2.COLOR_RGB2BGR))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894dd74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /workspace/Jaasia/Fish_Detection/explainable-ai/../data/images/test/00000000_jpg.rf.5d9188ebfd1f9ae1b9989c8ffc6bd7c4.jpg: 448x640 1 shark, 24.5ms\n",
      "Speed: 1.0ms preprocess, 24.5ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'penguin' not detected!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bbox \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_class_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m not detected!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Crop bbox\u001b[39;00m\n\u001b[32m     48\u001b[39m img = cv2.imread(image_path)\n",
      "\u001b[31mValueError\u001b[39m: 'penguin' not detected!"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "image_path = \"../data/images/test/00000000_jpg.rf.5d9188ebfd1f9ae1b9989c8ffc6bd7c4.jpg\"\n",
    "target_class_name = \"penguin\"\n",
    "\n",
    "# Load YOLOv8 model\n",
    "model = YOLO(\"../models/fish_yolov86/weights/best.pt\")\n",
    "device = next(model.model.parameters()).device\n",
    "\n",
    "# --------------------------\n",
    "# Forward hook for GradCAM\n",
    "# --------------------------\n",
    "layer_name = \"model.16.conv\"\n",
    "target_layer = dict(model.model.named_modules())[layer_name]\n",
    "activations, gradients = {}, {}\n",
    "\n",
    "def forward_hook(module, input, output):\n",
    "    activations['feat'] = output\n",
    "\n",
    "def backward_hook(module, grad_input, grad_output):\n",
    "    gradients['grad'] = grad_output[0]\n",
    "\n",
    "f_hook = target_layer.register_forward_hook(forward_hook)\n",
    "b_hook = target_layer.register_full_backward_hook(backward_hook)\n",
    "\n",
    "# --------------------------\n",
    "# Detect object and crop bbox\n",
    "# --------------------------\n",
    "results = model.predict(image_path, imgsz=640)\n",
    "names = model.names\n",
    "\n",
    "bbox = None\n",
    "for det in results[0].boxes:\n",
    "    cls_id = int(det.cls[0])\n",
    "    if names[cls_id] == target_class_name:\n",
    "        bbox = det.xyxy[0].cpu().numpy().astype(int)\n",
    "        break\n",
    "\n",
    "if bbox is None:\n",
    "    raise ValueError(f\"'{target_class_name}' not detected!\")\n",
    "\n",
    "# Crop bbox\n",
    "img = cv2.imread(image_path)\n",
    "x1, y1, x2, y2 = bbox\n",
    "cropped_img = img[y1:y2, x1:x2]\n",
    "\n",
    "# --------------------------\n",
    "# Preprocess for model\n",
    "# --------------------------\n",
    "cropped_rgb = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n",
    "cropped_rgb = cv2.resize(cropped_rgb, (640, 640))\n",
    "x = torch.from_numpy(cropped_rgb).float() / 255.0\n",
    "x = x.permute(2,0,1).unsqueeze(0).to(device)\n",
    "x.requires_grad_(True)\n",
    "\n",
    "# --------------------------\n",
    "# Forward & backward\n",
    "# --------------------------\n",
    "outputs = model.model(x)  # direct forward\n",
    "score = outputs[0].mean()  # simple scalar for GradCAM\n",
    "model.model.zero_grad()\n",
    "score.backward()  # now gradients are stored\n",
    "\n",
    "# --------------------------\n",
    "# Compute GradCAM score (mean of gradients)\n",
    "# --------------------------\n",
    "grad_map = gradients['grad'][0]  # [C,H,W]\n",
    "feat_map = activations['feat'][0]  # [C,H,W]\n",
    "pooled_grads = torch.mean(grad_map, dim=(1,2))\n",
    "gradcam_score = torch.sum(pooled_grads * torch.mean(feat_map, dim=(1,2))).item()\n",
    "\n",
    "print(f\"GradCAM score for '{target_class_name}': {gradcam_score:.6f}\")\n",
    "\n",
    "# Remove hooks\n",
    "f_hook.remove()\n",
    "b_hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde6a55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.0.conv Conv2d(3, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.1.conv Conv2d(48, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.2.cv1.conv Conv2d(96, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.2.cv2.conv Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.2.m.0.cv1.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.2.m.0.cv2.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.2.m.1.cv1.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.2.m.1.cv2.conv Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.3.conv Conv2d(96, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.4.cv1.conv Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.4.cv2.conv Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.4.m.0.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.0.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.1.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.1.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.2.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.2.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.3.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.4.m.3.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.5.conv Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.6.cv1.conv Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.6.cv2.conv Conv2d(1152, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.6.m.0.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.0.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.1.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.1.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.2.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.2.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.3.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.6.m.3.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.7.conv Conv2d(384, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.8.cv1.conv Conv2d(576, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.8.cv2.conv Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.8.m.0.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.8.m.0.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.8.m.1.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.8.m.1.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.9.cv1.conv Conv2d(576, 288, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.9.cv2.conv Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.12.cv1.conv Conv2d(960, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.12.cv2.conv Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.12.m.0.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.12.m.0.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.12.m.1.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.12.m.1.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.cv1.conv Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.15.cv2.conv Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.15.m.0.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.m.0.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.m.1.cv1.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.15.m.1.cv2.conv Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.16.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.18.cv1.conv Conv2d(576, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.18.cv2.conv Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.18.m.0.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.18.m.0.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.18.m.1.cv1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.18.m.1.cv2.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.19.conv Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "model.21.cv1.conv Conv2d(960, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.21.cv2.conv Conv2d(1152, 576, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.21.m.0.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.21.m.0.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.21.m.1.cv1.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.21.m.1.cv2.conv Conv2d(288, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.0.0.conv Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.0.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.0.2 Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv2.1.0.conv Conv2d(384, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.1.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.1.2 Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv2.2.0.conv Conv2d(576, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.2.1.conv Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv2.2.2 Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv3.0.0.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.0.1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.0.2 Conv2d(192, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv3.1.0.conv Conv2d(384, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.1.1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.1.2 Conv2d(192, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.cv3.2.0.conv Conv2d(576, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.2.1.conv Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "model.22.cv3.2.2 Conv2d(192, 7, kernel_size=(1, 1), stride=(1, 1))\n",
      "model.22.dfl.conv Conv2d(16, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.model.named_modules():\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        print(name, layer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fish",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
